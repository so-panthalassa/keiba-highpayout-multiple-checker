{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6b64f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import numpy as np, json, math\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import sklearn\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (average_precision_score, brier_score_loss,\n",
    "                             roc_auc_score)\n",
    "from sklearn.model_selection import (GroupKFold, StratifiedGroupKFold,\n",
    "                                     StratifiedKFold)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a4213",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'MS Gothic'\n",
    "results_df = pd.read_pickle('02_horse_results_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測に無関係なカラムの削除\n",
    "results_df = results_df.drop([\n",
    "    \"馬名\",\"horse_id\",\"着順\",\"タイム\",\"着差\",\"人気\",\"date\",\n",
    "    \n",
    "    \"HighPayoutRace\",\"jockey_id\"\n",
    "], axis=1, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38bc64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重複カラムの削除\n",
    "results_df = results_df.drop([\"距離区分\",\"年齢_bin\",\"斤量区分\",\"馬体重_bin\",\"体重_bin\",\"性齢\",\"馬番\"], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d801613f",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd43ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = \"HighPayoutHorse\"\n",
    "RACE_COL  = \"race_id\"\n",
    "DROP_COLS = [RACE_COL, \"着順_num\", \"複勝フラグ\", \"単勝\"]\n",
    "\n",
    "# 学習でカテゴリ扱いする列（存在チェックは都度する）\n",
    "BASE_CAT_COLS = [\n",
    "    \"騎手\",\"調教師\",\"weather\",\"race_type\",\"ground_state\",\n",
    "    \"競馬場\",\"所属\",\"月\",\"性\",\"枠番\",\"最内枠\",\"大外枠\"\n",
    "]\n",
    "\n",
    "NUMERIC_FORCE = [\"月\", \"枠番\", \"最内枠\", \"大外枠\"]\n",
    "\n",
    "def lump_rare(s: pd.Series, min_count=50, other=\"__OTHER__\"):\n",
    "    vc = s.value_counts(dropna=False)\n",
    "    rares = set(vc[vc < min_count].index)\n",
    "    return s.astype(str).where(~s.astype(str).isin(rares), other)\n",
    "\n",
    "def normalize_text(s: pd.Series) -> pd.Series:\n",
    "    return (s.astype(\"string\")\n",
    "              .str.normalize(\"NFKC\")\n",
    "              .str.strip()\n",
    "              .str.replace(r\"\\s+\", \" \", regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_core_features(df: pd.DataFrame,\n",
    "                        drop_cols=DROP_COLS,\n",
    "                        label_col=LABEL_COL,\n",
    "                        base_cat_cols=BASE_CAT_COLS,\n",
    "                        min_count_for_lump=50):\n",
    "    df = df.copy()\n",
    "\n",
    "    feat_cols = [c for c in df.columns if c not in drop_cols + [label_col]]\n",
    "    X = df[feat_cols].copy()\n",
    "    y = df[label_col].astype(int).to_numpy().ravel()\n",
    "    groups = df[RACE_COL].astype(str).to_numpy().ravel()\n",
    "\n",
    "    # 存在するカテゴリ候補だけ採用\n",
    "    cat_cols = [c for c in base_cat_cols if c in X.columns]\n",
    "    # 数値強制のものはカテゴリ対象から外す\n",
    "    cat_cols = [c for c in cat_cols if c not in NUMERIC_FORCE]\n",
    "\n",
    "    # 低頻度カテゴリまとめ + 軽い正規化\n",
    "    for c in cat_cols:\n",
    "        X[c] = normalize_text(X[c])\n",
    "        X[c] = lump_rare(X[c], min_count=min_count_for_lump, other=\"__OTHER__\")\n",
    "\n",
    "    # 数値化（カテゴリ以外はなるべく float に）\n",
    "    for c in X.columns:\n",
    "        if c not in cat_cols:\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "\n",
    "    return X, y, groups, cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9bc223",
   "metadata": {},
   "source": [
    "## 推奨設定にて学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4ce1b416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_lgbm(X_core: pd.DataFrame, cat_cols: list[str], levels_json=\"03_lgbm_cat_levels.json\"):\n",
    "    X_lgbm = X_core.copy()\n",
    "\n",
    "    # JSON が存在すれば学習語彙を読み込み（推論時はこちらを必ず使う）\n",
    "    if os.path.exists(levels_json):\n",
    "        obj = json.loads(Path(levels_json).read_text(encoding=\"utf-8\"))\n",
    "        levels = obj[\"levels\"]  # {col: [cat1, cat2, ...]}\n",
    "\n",
    "        # 学習時の語彙で dtype 固定（未知は __OTHER__ 扱い→カテゴリに __OTHER__ が含まれている前提）\n",
    "        for c in cat_cols:\n",
    "            if c in X_lgbm.columns and c in levels:\n",
    "                vals = normalize_text(X_lgbm[c]).fillna(\"__OTHER__\")\n",
    "                cats = levels[c]\n",
    "                X_lgbm[c] = vals.where(vals.isin(cats), \"__OTHER__\")\\\n",
    "                                .astype(pd.CategoricalDtype(categories=cats, ordered=False))\n",
    "    else:\n",
    "        # （学習時のみ実行される分岐）語彙を作って保存\n",
    "        levels = {}\n",
    "        for c in cat_cols:\n",
    "            vals = normalize_text(X_lgbm[c]).fillna(\"__OTHER__\")\n",
    "            uniq = [\"__OTHER__\"] + sorted([u for u in vals.unique().tolist() if u != \"__OTHER__\"])\n",
    "            X_lgbm[c] = vals.where(vals.isin(uniq), \"__OTHER__\")\\\n",
    "                            .astype(pd.CategoricalDtype(categories=uniq, ordered=False))\n",
    "            levels[c] = uniq\n",
    "        Path(levels_json).write_text(json.dumps({\"levels\": levels}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # 数値強制\n",
    "    for c in NUMERIC_FORCE:\n",
    "        if c in X_lgbm.columns:\n",
    "            X_lgbm[c] = pd.to_numeric(X_lgbm[c], errors=\"coerce\")\n",
    "\n",
    "    return X_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c7b1d340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.109995\n",
      "[Fold 1] PR-AUC=0.0432  ROC-AUC=0.6506  Brier=0.0229\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.105814\n",
      "[Fold 2] PR-AUC=0.0451  ROC-AUC=0.6906  Brier=0.0218\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.109301\n",
      "[Fold 3] PR-AUC=0.0375  ROC-AUC=0.6318  Brier=0.0224\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.106359\n",
      "[Fold 4] PR-AUC=0.0426  ROC-AUC=0.6569  Brier=0.0219\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.113797\n",
      "[Fold 5] PR-AUC=0.0445  ROC-AUC=0.6546  Brier=0.0240\n",
      "\n",
      "=== OOF ===\n",
      "ROC-AUC: 0.6569 (±0.0190)\n",
      "PR-AUC : 0.0426 (±0.0027)\n",
      "Brier  : 0.0226 (±0.0008)\n"
     ]
    }
   ],
   "source": [
    "X_core, y, groups, cat_cols = build_core_features(results_df)\n",
    "\n",
    "X_lgbm = prepare_for_lgbm(X_core, cat_cols, levels_json=\"03_lgbm_cat_levels.json\")\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_pred = np.zeros(len(X_lgbm))\n",
    "pr_list, roc_list, br_list = [], [], []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(cv.split(X_lgbm, y, groups=groups), 1):\n",
    "    X_tr, X_va = X_lgbm.iloc[tr_idx].copy(), X_lgbm.iloc[va_idx].copy()\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "    clf = LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        boosting_type=\"gbdt\",\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=4000,\n",
    "        num_leaves=255,\n",
    "        min_child_samples=10,\n",
    "        min_data_per_group=1,\n",
    "        cat_smooth=5,\n",
    "        subsample=0.8, colsample_bytree=0.8, max_bin=511,\n",
    "        is_unbalance=False,\n",
    "        scale_pos_weight=base_spw,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_va, y_va)], eval_metric=\"aucpr\",\n",
    "        categorical_feature=\"auto\",\n",
    "        callbacks=[early_stopping(300), log_evaluation(0)]\n",
    "    )\n",
    "\n",
    "    p = clf.predict_proba(X_va)[:, 1]\n",
    "    oof_pred[va_idx] = p\n",
    "\n",
    "    pr  = average_precision_score(y_va, p)\n",
    "    roc = roc_auc_score(y_va, p)\n",
    "    br  = brier_score_loss(y_va, p)\n",
    "    pr_list.append(pr); roc_list.append(roc); br_list.append(br)\n",
    "\n",
    "    print(f\"[Fold {fold}] PR-AUC={pr:.4f}  ROC-AUC={roc:.4f}  Brier={br:.4f}\")\n",
    "\n",
    "print(\"\\n=== OOF ===\")\n",
    "print(f\"ROC-AUC: {np.mean(roc_list):.4f} (±{np.std(roc_list):.4f})\")\n",
    "print(f\"PR-AUC : {np.mean(pr_list):.4f} (±{np.std(pr_list):.4f})\")\n",
    "print(f\"Brier  : {np.mean(br_list):.4f} (±{np.std(br_list):.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b91086",
   "metadata": {},
   "source": [
    "ROC-AUC : 分類性能(最大１)  \n",
    "PR-AUC  : 陽性クラスの見分け性能（0.2~0.3あたりを目指す）  \n",
    "Brier   : 二乗誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e6759c",
   "metadata": {},
   "source": [
    "## ハイパーパラメータの探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83e8c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4dcc7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"n_estimators\": 5000,  # early stopping で制御\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 10.0),\n",
    "        \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", base_spw*0.5, base_spw*2.0, log=True),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbosity\": -1,\n",
    "    }\n",
    "\n",
    "    pr_list = []\n",
    "\n",
    "    for tr_idx, va_idx in cv.split(X_core, y, groups=groups):\n",
    "        X_tr_core = X_core.iloc[tr_idx].copy()\n",
    "        X_va_core = X_core.iloc[va_idx].copy()\n",
    "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "        for c in cat_cols:\n",
    "            vals_tr = normalize_text(X_tr_core[c]).fillna(\"__OTHER__\")\n",
    "            cats = [\"__OTHER__\"] + sorted([u for u in pd.Series(vals_tr).unique().tolist() if u != \"__OTHER__\"])\n",
    "            X_tr_core[c] = vals_tr.where(vals_tr.isin(cats), \"__OTHER__\").astype(\n",
    "                pd.CategoricalDtype(categories=cats, ordered=False)\n",
    "            )\n",
    "            vals_va = normalize_text(X_va_core[c]).fillna(\"__OTHER__\")\n",
    "            X_va_core[c] = vals_va.where(vals_va.isin(cats), \"__OTHER__\").astype(\n",
    "                pd.CategoricalDtype(categories=cats, ordered=False)\n",
    "            )\n",
    "        for c in NUMERIC_FORCE:\n",
    "            if c in X_tr_core.columns:\n",
    "                X_tr_core[c] = pd.to_numeric(X_tr_core[c], errors=\"coerce\")\n",
    "            if c in X_va_core.columns:\n",
    "                X_va_core[c] = pd.to_numeric(X_va_core[c], errors=\"coerce\")\n",
    "\n",
    "        clf = LGBMClassifier(**params)\n",
    "        clf.fit(\n",
    "            X_tr_core, y_tr,\n",
    "            eval_set=[(X_va_core, y_va)],\n",
    "            eval_metric=\"aucpr\",\n",
    "            categorical_feature=\"auto\",\n",
    "            callbacks=[early_stopping(200), log_evaluation(-1)]\n",
    "        )\n",
    "\n",
    "        p = clf.predict_proba(X_va_core)[:, 1]\n",
    "        pr_list.append(average_precision_score(y_va, p))\n",
    "\n",
    "    return float(np.mean(pr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "47c68a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.103413\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0997373\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.100894\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.100025\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.106854\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.114511\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.112423\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.113292\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.112545\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.117743\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.123709\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.122065\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.121945\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.120337\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.12713\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.103499\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0996897\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.100991\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.100132\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.107137\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102632\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0987044\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0991307\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.105906\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.103439\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.100019\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.10125\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.100066\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.106938\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[251]\tvalid_0's binary_logloss: 0.157328\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[291]\tvalid_0's binary_logloss: 0.147241\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[275]\tvalid_0's binary_logloss: 0.160143\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[300]\tvalid_0's binary_logloss: 0.153592\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[246]\tvalid_0's binary_logloss: 0.159773\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.108424\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.105826\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.106792\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.105883\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.111877\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102881\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0992275\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.100557\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0996366\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.106396\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.185986\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.184647\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.188583\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.185858\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[494]\tvalid_0's binary_logloss: 0.15404\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.1023\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0984369\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.099876\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0990634\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.105782\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.102058\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0982664\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0999602\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0988927\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.105537\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.102467\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0984849\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.100036\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0988801\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.105709\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.102685\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0985868\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0998183\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0992153\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.105832\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.102345\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0984789\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0998433\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0988892\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.105385\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102214\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.098602\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0999025\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0988653\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.10594\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102166\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0986134\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0999502\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.098814\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.105898\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102319\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0987647\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.099976\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0990616\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.105807\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.103339\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.099896\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.101036\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.100161\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.107052\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.102583\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.0986863\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.100157\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.0992274\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.106303\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.102539\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.0987536\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.0999136\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.0989177\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.106069\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.102759\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0986747\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0991569\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.105957\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102882\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0991167\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.100239\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0993868\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.106146\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102928\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0989132\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.100198\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0993703\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.106146\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.102642\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.0987742\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.100006\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.0989155\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.106035\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102545\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0984207\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0999572\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0985735\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.10552\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.103049\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0994096\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.100522\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0996278\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.106578\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.102419\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.098376\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0998833\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0987527\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.105476\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.102297\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0983236\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0997757\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0988446\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.105424\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.10236\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.0985758\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.0997339\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.0989465\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.105677\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.1028\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0990212\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.100274\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.099447\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.106172\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.102521\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0983082\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.0998238\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0987942\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.105488\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.102353\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.0983821\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0999241\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0989204\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.105594\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.139349\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.138191\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.137683\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.13685\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.141895\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.102464\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.0982787\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.0998158\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.0988919\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.105598\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.102542\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0983259\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0997389\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0987255\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.105487\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102526\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0983312\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0998883\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0988594\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.105569\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.104109\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.100502\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.102065\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.100482\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.107423\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.102382\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0984556\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0998023\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0990073\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.10563\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.10288\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0993811\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.100311\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0994149\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.106213\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102611\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0988052\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.100284\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0994483\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.106059\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.10239\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.0984129\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.0997206\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.0988515\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.105558\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.10257\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0987179\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0998879\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0991959\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.105685\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.102563\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0985952\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0999018\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0986951\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.105748\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.1025\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.098366\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0997643\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0987816\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.105536\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102372\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0985154\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0998806\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0987857\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.105559\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.102909\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0993867\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.100492\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0995397\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.106485\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.102687\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.098888\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.100266\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's binary_logloss: 0.0993166\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.106313\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.102473\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0984748\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0998619\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0987567\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's binary_logloss: 0.105339\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.10262\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.0987309\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's binary_logloss: 0.099937\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.0989685\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid_0's binary_logloss: 0.105925\n",
      "Best PR-AUC: 0.05989009323747061\n",
      "Best params:\n",
      "  learning_rate: 0.010058869890215366\n",
      "  num_leaves: 228\n",
      "  max_depth: 6\n",
      "  min_child_samples: 91\n",
      "  subsample: 0.7866868808729768\n",
      "  colsample_bytree: 0.9425139611922944\n",
      "  reg_alpha: 3.820653140992346\n",
      "  reg_lambda: 8.07239384700518\n",
      "  scale_pos_weight: 22.43416364879291\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", study_name=\"lgbm_pr_auc\")\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=False)\n",
    "\n",
    "print(\"Best PR-AUC:\", study.best_value)\n",
    "print(\"Best params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bd00481a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved best_params -> 03_lgbm_best_params.json\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params | {\n",
    "    \"objective\": \"binary\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"n_estimators\": 5000,  # early stoppingで実質決まる\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "# JSON 保存（最適パラメタ）\n",
    "with open(\"03_lgbm_best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_params, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ saved best_params -> 03_lgbm_best_params.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b75612",
   "metadata": {},
   "source": [
    "## best_paramsで再学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4992fba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.1025\n",
      "[Fold 1] PR-AUC=0.0562  ROC-AUC=0.7738  Brier=0.0217  best_iter=5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.098366\n",
      "[Fold 2] PR-AUC=0.0600  ROC-AUC=0.7990  Brier=0.0205  best_iter=5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0997643\n",
      "[Fold 3] PR-AUC=0.0601  ROC-AUC=0.7895  Brier=0.0209  best_iter=5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's binary_logloss: 0.0987816\n",
      "[Fold 4] PR-AUC=0.0590  ROC-AUC=0.7980  Brier=0.0207  best_iter=5\n",
      "Training until validation scores don't improve for 300 rounds\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's binary_logloss: 0.105536\n",
      "[Fold 5] PR-AUC=0.0642  ROC-AUC=0.7889  Brier=0.0227  best_iter=6\n",
      "\n",
      "=== OOF ===\n",
      "PR-AUC : 0.0599 (±0.0025)\n",
      "ROC-AUC: 0.7898 (±0.0091)\n",
      "Brier  : 0.0213 (±0.0008)\n"
     ]
    }
   ],
   "source": [
    "with open(\"03_lgbm_best_params.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "oof_pred = np.zeros(len(X_core))\n",
    "pr_list, roc_list, br_list, best_iters = [], [], [], []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(cv.split(X_core, y, groups=groups), 1):\n",
    "    X_tr_core = X_core.iloc[tr_idx].copy()\n",
    "    X_va_core = X_core.iloc[va_idx].copy()\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "    # fold内で学習側から語彙→学習/検証へ適用\n",
    "    for c in cat_cols:\n",
    "        vals_tr = normalize_text(X_tr_core[c]).fillna(\"__OTHER__\")\n",
    "        cats = [\"__OTHER__\"] + sorted([u for u in pd.Series(vals_tr).unique().tolist() if u != \"__OTHER__\"])\n",
    "        X_tr_core[c] = vals_tr.where(vals_tr.isin(cats), \"__OTHER__\").astype(\n",
    "            pd.CategoricalDtype(categories=cats, ordered=False)\n",
    "        )\n",
    "        vals_va = normalize_text(X_va_core[c]).fillna(\"__OTHER__\")\n",
    "        X_va_core[c] = vals_va.where(vals_va.isin(cats), \"__OTHER__\").astype(\n",
    "            pd.CategoricalDtype(categories=cats, ordered=False)\n",
    "        )\n",
    "\n",
    "    for c in NUMERIC_FORCE:\n",
    "        if c in X_tr_core.columns:\n",
    "            X_tr_core[c] = pd.to_numeric(X_tr_core[c], errors=\"coerce\")\n",
    "        if c in X_va_core.columns:\n",
    "            X_va_core[c] = pd.to_numeric(X_va_core[c], errors=\"coerce\")\n",
    "\n",
    "    clf = LGBMClassifier(**best_params)\n",
    "    clf.fit(\n",
    "        X_tr_core, y_tr,\n",
    "        eval_metric=\"aucpr\",\n",
    "        eval_set=[(X_va_core, y_va)],\n",
    "        categorical_feature=\"auto\",\n",
    "        callbacks=[early_stopping(300), log_evaluation(0)]\n",
    "    )\n",
    "\n",
    "    p = clf.predict_proba(X_va_core)[:, 1]\n",
    "    oof_pred[va_idx] = p\n",
    "\n",
    "    pr  = average_precision_score(y_va, p)\n",
    "    roc = roc_auc_score(y_va, p)\n",
    "    br  = brier_score_loss(y_va, p)\n",
    "    pr_list.append(pr); roc_list.append(roc); br_list.append(br)\n",
    "    best_iters.append(getattr(clf, \"best_iteration_\", clf.n_estimators))\n",
    "    print(f\"[Fold {fold}] PR-AUC={pr:.4f}  ROC-AUC={roc:.4f}  Brier={br:.4f}  best_iter={best_iters[-1]}\")\n",
    "\n",
    "print(\"\\n=== OOF ===\")\n",
    "print(f\"PR-AUC : {np.mean(pr_list):.4f} (±{np.std(pr_list):.4f})\")\n",
    "print(f\"ROC-AUC: {np.mean(roc_list):.4f} (±{np.std(roc_list):.4f})\")\n",
    "print(f\"Brier  : {np.mean(br_list):.4f} (±{np.std(br_list):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c013ca",
   "metadata": {},
   "source": [
    "最終学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "874bb46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved -> 03_lgbm_nativecat_odds_tuned.joblib\n",
      "✅ saved -> 03_lgbm_best_params_final.json\n",
      "✅ saved -> 03_lgbm_feature_cols.json\n"
     ]
    }
   ],
   "source": [
    "final_n_estimators = int(np.median(best_iters) * 1.1)\n",
    "final_params = best_params.copy()\n",
    "final_params[\"n_estimators\"] = max(final_n_estimators, 100)\n",
    "\n",
    "# 最終学習では「専用関数」で語彙を固定＆保存（→推論時に再現）\n",
    "X_full_lgbm = prepare_for_lgbm(X_core, cat_cols, levels_json=\"03_lgbm_cat_levels.json\")\n",
    "\n",
    "model_lgbm = LGBMClassifier(**final_params)\n",
    "model_lgbm.fit(\n",
    "    X_full_lgbm, y,\n",
    "    categorical_feature=\"auto\"\n",
    ")\n",
    "\n",
    "model_path = \"03_lgbm_nativecat_odds_tuned.joblib\"\n",
    "joblib.dump(model_lgbm, model_path)\n",
    "print(f\"✅ saved -> {model_path}\")\n",
    "\n",
    "with open(\"03_lgbm_best_params_final.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_params, f, ensure_ascii=False, indent=2)\n",
    "print(\"✅ saved -> 03_lgbm_best_params_final.json\")\n",
    "\n",
    "# 特徴量メタ保存（列名は最終学習に使ったフレームで確定）\n",
    "FEATURES_JSON = Path(\"03_lgbm_feature_cols.json\")\n",
    "FEATURES_JSON.write_text(\n",
    "    json.dumps({\n",
    "        \"features\": list(X_full_lgbm.columns),\n",
    "        \"categorical_features\": cat_cols,   # 実際にカテゴリ扱いした列集合\n",
    "        \"numeric_force\": NUMERIC_FORCE\n",
    "    }, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "print(\"✅ saved -> 03_lgbm_feature_cols.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a1cf8",
   "metadata": {},
   "source": [
    "## 特徴量重要度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b5516924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Importance (top20 by gain) ===\n",
      "          feature          gain      gain%\n",
      "31        単勝_最人気差  1.528871e+06  55.125890\n",
      "4             調教師  5.589224e+05  20.152838\n",
      "2              騎手  3.729306e+05  13.446607\n",
      "28          log単勝  1.707084e+05   6.155164\n",
      "32       単勝_2番人気差  5.867800e+04   2.115729\n",
      "29          単勝ランク  4.803012e+04   1.731802\n",
      "30          単勝pct  6.266634e+03   0.225953\n",
      "25    馬_コース適性_複勝率  3.435685e+03   0.123879\n",
      "22    馬_直近5走_平均着順  3.408327e+03   0.122893\n",
      "3             馬体重  3.139814e+03   0.113211\n",
      "27  コンビ_直近50走_複勝率  2.962522e+03   0.106818\n",
      "14             年齢  2.187922e+03   0.078889\n",
      "21    馬_直近10走_複勝率  2.177227e+03   0.078503\n",
      "23   騎手_直近30走_複勝率  2.130293e+03   0.076811\n",
      "16          相対枠位置  1.952841e+03   0.070413\n",
      "9             競馬場  1.439497e+03   0.051903\n",
      "26     馬_距離適性_複勝率  1.257945e+03   0.045357\n",
      "12         馬体重_増減  1.113839e+03   0.040161\n",
      "15           出走頭数  1.049968e+03   0.037858\n",
      "1              斤量  6.250570e+02   0.022537\n"
     ]
    }
   ],
   "source": [
    "imp = pd.DataFrame({\"feature\": X_full_lgbm.columns, \"gain\": model_lgbm.booster_.feature_importance(\"gain\")})\n",
    "imp[\"gain%\"] = imp[\"gain\"] / imp[\"gain\"].sum() * 100\n",
    "print(\"\\n=== Feature Importance (top20 by gain) ===\")\n",
    "print(imp.sort_values(\"gain\", ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c741a86",
   "metadata": {},
   "source": [
    "現状人気のなさが大きく特徴量として寄与してしまっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "10c3bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_feat_cols = list(X_tr.columns)\n",
    "with open(\"03_lgbm_cat_levels.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({\"levels\": levels}, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc22274b",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3b811f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_catboost(X_core: pd.DataFrame, cat_cols: list[str], catcols_json=\"03_catboost_cat_cols.json\"):\n",
    "    X_cat = X_core.copy()\n",
    "\n",
    "    # CatBoost は str + cat_features=index が基本\n",
    "    for c in cat_cols:\n",
    "        X_cat[c] = normalize_text(X_cat[c]).fillna(\"NA\").astype(str)\n",
    "\n",
    "    cat_idx = [X_cat.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "    # 推論側で再現できるよう列名保存（※cat_idxは列順依存なので列名を保存）\n",
    "    Path(catcols_json).write_text(json.dumps({\"categorical_features\": cat_cols}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    return X_cat, cat_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb99597",
   "metadata": {},
   "source": [
    "## Optunaでハイパーパラメータ探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dd99056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_core, y, groups, cat_cols = build_core_features(results_df)\n",
    "\n",
    "X_cat, cat_idx = prepare_for_catboost(X_core, cat_cols, catcols_json=\"03_catboost_cat_cols.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "af633656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0, log=True),\n",
    "        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 5.0),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 0.0, 5.0),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 128, 254),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"rsm\": trial.suggest_float(\"rsm\", 0.6, 1.0),\n",
    "        \"iterations\": 5000,\n",
    "        \"class_weights\": [\n",
    "            1.0,\n",
    "            trial.suggest_float(\"class_weight_pos\", base_w1*0.5, base_w1*2.0, log=True)\n",
    "        ],\n",
    "        \"random_state\": 42,\n",
    "        \"verbose\": False,\n",
    "        \"allow_writing_files\": False,\n",
    "        \"task_type\": \"CPU\",\n",
    "    }\n",
    "\n",
    "    pr_list = []\n",
    "    for tr_idx, va_idx in cv.split(X_cat, y, groups=groups):\n",
    "        X_tr, X_va = X_cat.iloc[tr_idx], X_cat.iloc[va_idx]\n",
    "        y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "        train_pool = Pool(X_tr, y_tr, cat_features=cat_idx)\n",
    "        valid_pool = Pool(X_va, y_va, cat_features=cat_idx)\n",
    "\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=300, verbose=False)\n",
    "\n",
    "        p = model.predict_proba(valid_pool)[:, 1]\n",
    "        pr_list.append(average_precision_score(y_va, p))\n",
    "\n",
    "    return float(np.mean(pr_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bb0f7744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-16 00:26:46,539] Trial 0 failed with parameters: {'learning_rate': 0.13917639709769958, 'depth': 8, 'l2_leaf_reg': 2.328211025539983, 'bagging_temperature': 0.6259699097796922, 'random_strength': 4.1822576950455295, 'border_count': 130, 'subsample': 0.9194065132481472, 'rsm': 0.7034879611916857, 'class_weight_pos': 76.0743984362791} because of the following error: KeyboardInterrupt('').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\soich\\AppData\\Local\\Temp\\ipykernel_26348\\2491112914.py\", line 34, in objective\n",
      "    model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=300, verbose=False)\n",
      "  File \"c:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\catboost\\core.py\", line 5245, in fit\n",
      "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "  File \"c:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
      "    self._train(\n",
      "  File \"c:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 5023, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 5072, in _catboost._CatBoost._train\n",
      "KeyboardInterrupt\n",
      "[W 2025-09-16 00:26:46,556] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m, study_name=\u001b[33m\"\u001b[39m\u001b[33mcatboost_pr_auc\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest PR-AUC:\u001b[39m\u001b[33m\"\u001b[39m, study.best_value)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest params:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\optuna\\study\\_optimize.py:258\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    254\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    255\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    257\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     31\u001b[39m valid_pool = Pool(X_va, y_va, cat_features=cat_idx)\n\u001b[32m     33\u001b[39m model = CatBoostClassifier(**params)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m p = model.predict_proba(valid_pool)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     37\u001b[39m pr_list.append(average_precision_score(y_va, p))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\catboost\\core.py:5245\u001b[39m, in \u001b[36mCatBoostClassifier.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5243\u001b[39m     CatBoostClassifier._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5246\u001b[39m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5247\u001b[39m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\catboost\\core.py:2410\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2407\u001b[39m allow_clear_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mallow_clear_pool\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[33m'\u001b[39m\u001b[33mTraining plots\u001b[39m\u001b[33m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m.get_params())]):\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_sets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[32m   2419\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._object._get_loss_function_name()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\project\\horse_ana\\.venv4\\Lib\\site-packages\\catboost\\core.py:1790\u001b[39m, in \u001b[36m_CatBoostBase._train\u001b[39m\u001b[34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_trained_model_attributes()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5023\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5072\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", study_name=\"catboost_pr_auc\")\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"Best PR-AUC:\", study.best_value)\n",
    "print(\"Best params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "Path(\"03_cat_best_params_search.json\").write_text(\n",
    "    json.dumps(study.best_params, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "print(\"✅ saved -> 03_cat_best_params_search.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f126d75",
   "metadata": {},
   "source": [
    "## 探索済みパラメータで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0b44d613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "再学習ベース params: {'loss_function': 'Logloss', 'eval_metric': 'AUC', 'learning_rate': 0.03076622576045712, 'depth': 4, 'l2_leaf_reg': 2.692607180834445, 'bagging_temperature': 1.6362128720538305, 'random_strength': 3.197491890827852, 'border_count': 192, 'subsample': 0.7688116926449186, 'rsm': 0.716035050229886, 'iterations': 5000, 'class_weights': [1.0, 24.31116790970121], 'random_state': 42, 'verbose': False, 'allow_writing_files': False, 'task_type': 'CPU'}\n"
     ]
    }
   ],
   "source": [
    "SEARCH_JSON = Path(\"03_cat_best_params_search.json\")\n",
    "best_search = json.loads(SEARCH_JSON.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "class_weight_pos = float(best_search.pop(\"class_weight_pos\"))\n",
    "\n",
    "best_params_cat = {\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"eval_metric\": \"AUC\",\n",
    "    \"learning_rate\": best_search[\"learning_rate\"],\n",
    "    \"depth\": int(best_search[\"depth\"]),\n",
    "    \"l2_leaf_reg\": float(best_search[\"l2_leaf_reg\"]),\n",
    "    \"bagging_temperature\": float(best_search[\"bagging_temperature\"]),\n",
    "    \"random_strength\": float(best_search[\"random_strength\"]),\n",
    "    \"border_count\": int(best_search[\"border_count\"]),\n",
    "    \"subsample\": float(best_search[\"subsample\"]),\n",
    "    \"rsm\": float(best_search[\"rsm\"]),\n",
    "    \"iterations\": 5000,\n",
    "    \"class_weights\": [1.0, class_weight_pos],\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": False,\n",
    "    \"allow_writing_files\": False,\n",
    "    \"task_type\": \"CPU\",\n",
    "}\n",
    "print(\"再学習ベース params:\", best_params_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "22a41986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_idx (by X_core): [2, 4, 6, 7, 8, 9, 10, 13] ... count= 8\n"
     ]
    }
   ],
   "source": [
    "X_core, y, groups, cat_cols = build_core_features(results_df)\n",
    "\n",
    "X_cb = X_core.copy()\n",
    "for c in cat_cols:\n",
    "    if c in X_cb.columns:\n",
    "        X_cb[c] = X_cb[c].astype(str)\n",
    "\n",
    "cat_idx = [X_cb.columns.get_loc(c) for c in cat_cols]\n",
    "\n",
    "print(\"cat_idx (by X_core):\", cat_idx[:10], \"...\", \"count=\", len(cat_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd6aa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] PR-AUC=0.0676  ROC-AUC=0.7962  Brier=0.1180  best_iter=797\n",
      "[Fold 2] PR-AUC=0.0702  ROC-AUC=0.8092  Brier=0.1250  best_iter=245\n",
      "[Fold 3] PR-AUC=0.0679  ROC-AUC=0.8092  Brier=0.1233  best_iter=84\n",
      "[Fold 4] PR-AUC=0.0649  ROC-AUC=0.8206  Brier=0.1247  best_iter=456\n",
      "[Fold 5] PR-AUC=0.0757  ROC-AUC=0.7996  Brier=0.1210  best_iter=528\n",
      "\n",
      "=== OOF (CatBoost) ===\n",
      "ROC-AUC: 0.8070 (±0.0086)\n",
      "PR-AUC : 0.0693 (±0.0036)\n",
      "Brier  : 0.1224 (±0.0026)\n"
     ]
    }
   ],
   "source": [
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_pred = np.zeros(len(X_cat))\n",
    "pr_list, roc_list, br_list, best_iters = [], [], [], []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(cv.split(X_cat, y, groups=groups), 1):\n",
    "    X_tr, X_va = X_cat.iloc[tr_idx].copy(), X_cat.iloc[va_idx].copy()\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "    # cat_features はインデックスで指定（推論時と一致する）\n",
    "    train_pool = Pool(X_tr, y_tr, cat_features=cat_idx)\n",
    "    valid_pool = Pool(X_va, y_va, cat_features=cat_idx)\n",
    "\n",
    "    model_cb = CatBoostClassifier(**best_params_cat)\n",
    "    model_cb.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=300, verbose=False)\n",
    "\n",
    "    p = model_cb.predict_proba(valid_pool)[:, 1]\n",
    "    oof_pred[va_idx] = p\n",
    "\n",
    "    pr  = average_precision_score(y_va, p)\n",
    "    roc = roc_auc_score(y_va, p)\n",
    "    br  = brier_score_loss(y_va, p)\n",
    "    pr_list.append(pr); roc_list.append(roc); br_list.append(br)\n",
    "\n",
    "    # CatBoost は best_iteration_ で最良イテレーションを持ってる\n",
    "    biter = getattr(model_cb, \"best_iteration_\", None)\n",
    "    if biter is None or biter <= 0:\n",
    "        biter = best_params_cat[\"iterations\"]\n",
    "    best_iters.append(biter)\n",
    "\n",
    "    print(f\"[Fold {fold}] PR-AUC={pr:.4f}  ROC-AUC={roc:.4f}  Brier={br:.4f}  best_iter={biter}\")\n",
    "\n",
    "print(\"\\n=== OOF (CatBoost) ===\")\n",
    "print(f\"ROC-AUC: {np.mean(roc_list):.4f} (±{np.std(roc_list):.4f})\")\n",
    "print(f\"PR-AUC : {np.mean(pr_list):.4f} (±{np.std(pr_list):.4f})\")\n",
    "print(f\"Brier  : {np.mean(br_list):.4f} (±{np.std(br_list):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "29abc42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_iterations: 300\n",
      "✅ saved CatBoost model -> 03_catboost_odds_features.cbm\n"
     ]
    }
   ],
   "source": [
    "final_iterations = int(np.median(best_iters) * 1.1)\n",
    "final_iterations = max(final_iterations, 300)\n",
    "print(\"final_iterations:\", final_iterations)\n",
    "\n",
    "final_params_cat = best_params_cat.copy()\n",
    "final_params_cat[\"iterations\"] = final_iterations\n",
    "\n",
    "full_pool = Pool(X_cat, y, cat_features=cat_idx)\n",
    "\n",
    "final_cat = CatBoostClassifier(**final_params_cat)\n",
    "final_cat.fit(full_pool, verbose=False)\n",
    "\n",
    "MODEL_CAT_PATH = \"03_catboost_odds_features.cbm\"\n",
    "final_cat.save_model(MODEL_CAT_PATH)\n",
    "print(f\"✅ saved CatBoost model -> {MODEL_CAT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6102d64",
   "metadata": {},
   "source": [
    "## 特徴量重要度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6eef8f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Feature Importance (CatBoost) ===\n",
      "          feature  importance  importance%\n",
      "31        単勝_最人気差   49.497445    49.497445\n",
      "28          log単勝   24.237612    24.237612\n",
      "30          単勝pct    8.413642     8.413642\n",
      "32       単勝_2番人気差    6.053179     6.053179\n",
      "29          単勝ランク    5.186599     5.186599\n",
      "19     馬_直近3走_複勝率    1.792135     1.792135\n",
      "20     馬_直近5走_複勝率    0.567343     0.567343\n",
      "26     馬_距離適性_複勝率    0.471470     0.471470\n",
      "25    馬_コース適性_複勝率    0.393779     0.393779\n",
      "22    馬_直近5走_平均着順    0.349495     0.349495\n",
      "14             年齢    0.338315     0.338315\n",
      "13              性    0.310886     0.310886\n",
      "23   騎手_直近30走_複勝率    0.271885     0.271885\n",
      "27  コンビ_直近50走_複勝率    0.216442     0.216442\n",
      "21    馬_直近10走_複勝率    0.191641     0.191641\n",
      "24  調教師_直近50走_複勝率    0.184585     0.184585\n",
      "7       race_type    0.174759     0.174759\n",
      "3             馬体重    0.160660     0.160660\n",
      "9             競馬場    0.145329     0.145329\n",
      "0              枠番    0.135269     0.135269\n",
      "12         馬体重_増減    0.115395     0.115395\n",
      "11              月    0.098817     0.098817\n",
      "5      course_len    0.091555     0.091555\n",
      "2              騎手    0.089851     0.089851\n",
      "8    ground_state    0.086731     0.086731\n",
      "4             調教師    0.082503     0.082503\n",
      "17            大外枠    0.076117     0.076117\n",
      "10             所属    0.062931     0.062931\n",
      "16          相対枠位置    0.061906     0.061906\n",
      "6         weather    0.044485     0.044485\n"
     ]
    }
   ],
   "source": [
    "imp_type = \"PredictionValuesChange\"  # or \"LossFunctionChange\"\n",
    "imp = model_cb.get_feature_importance(type=imp_type, data=Pool(X_cat, y, cat_features=cat_idx))\n",
    "\n",
    "feat_imp = pd.DataFrame({\"feature\": X_cat.columns, \"importance\": imp})\n",
    "feat_imp = feat_imp.sort_values(\"importance\", ascending=False)\n",
    "feat_imp[\"importance%\"] = 100 * feat_imp[\"importance\"] / feat_imp[\"importance\"].sum()\n",
    "\n",
    "print(\"\\n=== Feature Importance (CatBoost) ===\")\n",
    "print(feat_imp.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b30853",
   "metadata": {},
   "source": [
    "CatBoostにおいても単勝オッズが強く寄与してしまっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e9658cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徴量リストを保存しました -> 03_lgbm_feature_cols.json / 03_catboost_feature_cols.json\n"
     ]
    }
   ],
   "source": [
    "# --- CatBoost 用特徴量リスト保存 ---\n",
    "cat_feat_cols = list(X_tr.columns)\n",
    "with open(\"03_catboost_feature_cols.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cat_feat_cols, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"特徴量リストを保存しました -> 03_lgbm_feature_cols.json / 03_catboost_feature_cols.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBMと混ぜるときに使う推論関数\n",
    "def predict_catboost(df_new: pd.DataFrame, model_path=\"03_catboost_odds_features.cbm\",\n",
    "                     cat_cols_hint_path=\"03_catboost_cat_cols.json\"):\n",
    "    model_cb = CatBoostClassifier()\n",
    "    model_cb.load_model(model_path)\n",
    "    if os.path.exists(cat_cols_hint_path):\n",
    "        with open(cat_cols_hint_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cat_cols_hint = json.load(f)\n",
    "        cat_cols_use = [c for c in cat_cols_hint if c in df_new.columns]\n",
    "    else:\n",
    "        cat_cols_use = df_new.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    cat_idx_use = [df_new.columns.get_loc(c) for c in cat_cols_use]\n",
    "    pool = Pool(df_new, cat_features=cat_idx_use)\n",
    "    return model_cb.predict_proba(pool)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7cd53",
   "metadata": {},
   "source": [
    "# アンサンブル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc827e",
   "metadata": {},
   "source": [
    "学習用前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5cd137cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_PATH = \"03_lgbm_nativecat_odds_tuned.joblib\"\n",
    "CAT_PATH  = \"03_catboost_odds_features.cbm\" \n",
    "CAT_COLS_JSON = \"03_catboost_cat_cols.json\"\n",
    "LGBM_LEVELS_JSON = \"03_lgbm_cat_levels.json\"\n",
    "\n",
    "LABEL_COL = \"HighPayoutHorse\"\n",
    "RACE_COL  = \"race_id\"\n",
    "DROP_COLS = [RACE_COL, \"着順_num\", \"複勝フラグ\", \"単勝\"]\n",
    "\n",
    "TOP_K_PER_RACE = 2\n",
    "MIN_PROB = 0.165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b8bbfcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_core, y, groups, cat_cols = build_core_features(\n",
    "    results_df,\n",
    "    drop_cols=DROP_COLS,\n",
    "    label_col=LABEL_COL,\n",
    "    base_cat_cols=[\n",
    "        \"騎手\",\"調教師\",\"weather\",\"race_type\",\"ground_state\",\n",
    "        \"競馬場\",\"所属\",\"月\",\"性\",\"枠番\",\"最内枠\",\"大外枠\"\n",
    "    ],\n",
    "    min_count_for_lump=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cd3c4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM：語彙を JSON から“読むだけ”で固定（学習時に保存済み想定）\n",
    "X_lgbm = prepare_for_lgbm(X_core, cat_cols, levels_json=LGBM_LEVELS_JSON)\n",
    "\n",
    "# CatBoost：文字列カテゴリ＋ cat_idx（列順に依存）を用意\n",
    "X_cat, cat_idx = prepare_for_catboost(X_core, cat_cols, catcols_json=CAT_COLS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "83f14809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x15fb7c65310>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgbm = joblib.load(LGBM_PATH)\n",
    "model_cb   = CatBoostClassifier()\n",
    "model_cb.load_model(CAT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1ea0a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_to_model_features(df: pd.DataFrame, model) -> pd.DataFrame:\n",
    "    cols = getattr(model, \"feature_name_\", None) or getattr(model, \"feature_names_\", None)\n",
    "    return df.reindex(columns=list(cols)) if cols is not None else df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9cf4fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_lgbm = np.zeros(len(X_core))\n",
    "oof_cat  = np.zeros(len(X_core))\n",
    "\n",
    "for tr_idx, va_idx in cv.split(X_core, y, groups=groups):\n",
    "    # --- LGBM ---\n",
    "    X_va_lgbm = X_lgbm.iloc[va_idx]\n",
    "    X_va_lgbm = align_to_model_features(X_va_lgbm, model_lgbm)\n",
    "    oof_lgbm[va_idx] = model_lgbm.predict_proba(X_va_lgbm)[:, 1]\n",
    "\n",
    "    # --- CatBoost ---\n",
    "    X_va_cat = X_cat.iloc[va_idx]\n",
    "    # 列順が学習時とズレていたら整える（CatBoost でも一応合わせておく）\n",
    "    X_va_cat = align_to_model_features(X_va_cat, model_cb)\n",
    "    pool = Pool(X_va_cat, cat_features=cat_idx)\n",
    "    oof_cat[va_idx] = model_cb.predict_proba(pool)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1a971112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Blend search] best PR-AUC=0.1547 -> LGBM=1.00, Cat=0.00\n"
     ]
    }
   ],
   "source": [
    "# 最良の重みをグリッドサーチで探す\n",
    "grid = np.linspace(0.0, 1.0, 101) # LGBM0.0~1.0を101分割\n",
    "scores = [average_precision_score(y, w*oof_lgbm + (1-w)*oof_cat) for w in grid]\n",
    "W_LGBM = grid[int(np.argmax(scores))]\n",
    "W_CAT  = 1.0 - W_LGBM\n",
    "best_pr = max(scores)\n",
    "print(f\"[Blend search] best PR-AUC={best_pr:.4f} -> LGBM={W_LGBM:.2f}, Cat={W_CAT:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "42f2465f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OOF評価（CV外予測での公正評価）===\n",
      "ROC-AUC: 0.9174\n",
      "PR-AUC : 0.1547\n",
      "Brier  : 0.0473\n"
     ]
    }
   ],
   "source": [
    "p_blend_oof = W_LGBM * oof_lgbm + W_CAT * oof_cat\n",
    "\n",
    "print(\"\\n=== OOF評価（CV外予測での公正評価）===\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y, p_blend_oof):.4f}\")\n",
    "print(f\"PR-AUC : {average_precision_score(y, p_blend_oof):.4f}\")\n",
    "print(f\"Brier  : {brier_score_loss(y, p_blend_oof):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeabe2fa",
   "metadata": {},
   "source": [
    "## 買い目評価シミュレーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e35b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_picks(df_pred: pd.DataFrame, score_col: str, \n",
    "               race_col: str = RACE_COL, top_k: int = 2, \n",
    "               min_prob: float = MIN_PROB, \n",
    "               odds_min: float | None = None, odds_max: float | None = None):\n",
    "    tmp = df_pred[[race_col, score_col, \"単勝\"]].copy()\n",
    "    tmp[\"row_id\"] = df_pred.index\n",
    "\n",
    "    tmp = tmp.sort_values([race_col, score_col], ascending=[True, False])\n",
    "    topk = tmp.groupby(race_col, as_index=False).head(top_k)\n",
    "\n",
    "    ok_races = topk.groupby(race_col)[score_col].max()\n",
    "    ok_races = ok_races[ok_races >= min_prob].index\n",
    "    picks = topk[topk[race_col].isin(ok_races)].copy()\n",
    "\n",
    "    if odds_min is not None:\n",
    "        picks = picks[picks[\"単勝\"] >= odds_min]\n",
    "    if odds_max is not None:\n",
    "        picks = picks[picks[\"単勝\"] <= odds_max]\n",
    "\n",
    "    return picks[[\"row_id\", race_col, score_col, \"単勝\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_picks_counts(df_all: pd.DataFrame, picks: pd.DataFrame,\n",
    "                          race_col=RACE_COL, label_col=LABEL_COL):\n",
    "    pick_idx = picks[\"row_id\"].to_numpy()\n",
    "    bet_mask = df_all.index.isin(pick_idx)\n",
    "    n_bets = int(bet_mask.sum())\n",
    "    total_races = df_all[race_col].nunique()\n",
    "    bet_races = df_all.loc[bet_mask, race_col].nunique()\n",
    "    skipped = total_races - bet_races\n",
    "\n",
    "    # 馬単位の的中率\n",
    "    hit_rate_horse = float(df_all.loc[bet_mask, label_col].mean() or 0.0)\n",
    "\n",
    "    # レース単位の的中率（買った中に1頭でも当たり）\n",
    "    hit_rate_race = float(\n",
    "        df_all.loc[bet_mask, [race_col, label_col]]\n",
    "              .groupby(race_col)[label_col].max().mean() or 0.0\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        total_races=int(total_races),\n",
    "        bet_races=int(bet_races),\n",
    "        skipped=int(skipped),\n",
    "        n_bets=n_bets,\n",
    "        hit_rate_horse=hit_rate_horse,\n",
    "        hit_rate_race=hit_rate_race,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c59a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 買い目の評価（オッズ不使用） ===\n",
      "設定              : 上位2頭 / 確率しきい値 0.165\n",
      "総レース数        : 6788\n",
      "参加レース数      : 6723（参加率 99.0%）\n",
      "見送りレース数    : 65\n",
      "買い目点数        : 13446（1レース平均 2.00 点）\n",
      "的中率（馬単位）  : 0.101\n",
      "的中率（レース）  : 0.188 ＊そのレースで1頭でも当たりがいれば的中\n"
     ]
    }
   ],
   "source": [
    "pred_df = results_df.copy()\n",
    "pred_df[\"p_blend\"] = p_blend_oof\n",
    "\n",
    "picks = make_picks(pred_df, score_col=\"p_blend\",\n",
    "                   top_k=TOP_K_PER_RACE, min_prob=MIN_PROB)\n",
    "res = evaluate_picks_counts(pred_df, picks)\n",
    "\n",
    "total   = res[\"total_races\"]\n",
    "bet     = res[\"bet_races\"]\n",
    "skipped = res[\"skipped\"]\n",
    "n_bets  = res[\"n_bets\"]\n",
    "\n",
    "bet_rate         = (bet / total) if total else 0.0\n",
    "avg_bets_per_race = (n_bets / bet) if bet else 0.0\n",
    "\n",
    "print(\"\\n=== 買い目の評価（オッズ不使用） ===\")\n",
    "print(f\"設定              : 上位{TOP_K_PER_RACE}頭 / 確率しきい値 {MIN_PROB:.3f}\")\n",
    "print(f\"総レース数        : {total}\")\n",
    "print(f\"参加レース数      : {bet}（参加率 {bet_rate:.1%}）\")\n",
    "print(f\"見送りレース数    : {skipped}\")\n",
    "print(f\"買い目点数        : {n_bets}（1レース平均 {avg_bets_per_race:.2f} 点）\")\n",
    "print(f\"的中率（馬単位）  : {res['hit_rate_horse']:.3f}\")\n",
    "print(f\"的中率（レース）  : {res['hit_rate_race']:.3f} ＊そのレースで1頭でも当たりがいれば的中\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab57c49",
   "metadata": {},
   "source": [
    "オッズ系特徴（log単勝/単勝ランク 等）の寄与が高く、現在のしきい値(0.165)設定では参加率が99%と高止まり。  \n",
    "しきい値調整やオッズ寄与の抑制で選択性と精度の改善余地あり。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a12df0",
   "metadata": {},
   "source": [
    "## 妙味スコアと注目度の集計"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99045e5d",
   "metadata": {},
   "source": [
    "買い目判断材料として、馬ごとの妙味スコアとレースの注目度を出力できるようにする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "23a8a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved baselines -> 03_baselines.json : {'avg_myoumi_score_oof': 3.07846041491536, 'median_myoumi_score_oof': 1.1965446799067012, 'avg_race_attention_oof': 0.13285795279676615, 'median_race_attention_oof': 0.12906446699097196, 'n_races': 6788, 'n_horses': 92870}\n"
     ]
    }
   ],
   "source": [
    "df_oof = results_df.copy()\n",
    "df_oof[\"p_blend_oof\"] = oof_pred\n",
    "\n",
    "def _market_prob(s_odds: pd.Series) -> pd.Series:\n",
    "    if s_odds.notna().any():\n",
    "        inv = s_odds.apply(lambda x: 1/x if pd.notna(x) and x > 0 else np.nan)\n",
    "        denom = np.nansum(inv)\n",
    "        return inv / denom if denom and denom > 0 else pd.Series(np.nan, index=s_odds.index)\n",
    "    return pd.Series(np.nan, index=s_odds.index)\n",
    "\n",
    "if \"単勝\" in df_oof.columns:\n",
    "    df_oof[\"market_p\"] = df_oof.groupby(RACE_COL, group_keys=False)[\"単勝\"].transform(_market_prob)\n",
    "else:\n",
    "    df_oof[\"market_p\"] = np.nan\n",
    "\n",
    "eps = 1e-12\n",
    "has_market_any = df_oof[\"market_p\"].notna().any()\n",
    "\n",
    "if has_market_any:\n",
    "    df_oof[\"妙味スコア_oof\"] = df_oof[\"p_blend_oof\"] / (df_oof[\"market_p\"] + eps)\n",
    "else:\n",
    "    def _fallback_value(group: pd.DataFrame) -> pd.Series:\n",
    "        p = group[\"p_blend_oof\"].values\n",
    "        pos = np.clip(p - np.nanmean(p), 0, None)\n",
    "        vmax = np.nanmax(pos) if np.isfinite(pos).any() else np.nan\n",
    "        hv = (pos / vmax) if (vmax and vmax > 0) else np.zeros_like(pos)\n",
    "        return pd.Series(1.0 + 9.0 * hv, index=group.index)\n",
    "    df_oof[\"妙味スコア_oof\"] = df_oof.groupby(RACE_COL, group_keys=False).apply(_fallback_value)\n",
    "\n",
    "def _race_attention(g: pd.DataFrame) -> float:\n",
    "    top_p = float(np.nanmax(g[\"p_blend_oof\"].values)) if len(g) else np.nan\n",
    "\n",
    "    _eps = 1e-12\n",
    "    if g[\"market_p\"].notna().any():\n",
    "        pm = g[\"market_p\"].fillna(0).values\n",
    "        ent = -np.nansum(pm * np.log(pm + _eps))\n",
    "        ent_max = math.log(len(g)) if len(g) > 1 else 1.0\n",
    "        ent_norm = (ent / ent_max) if ent_max > 0 else 0.0\n",
    "    else:\n",
    "        ent_norm = 0.0\n",
    "\n",
    "    race_skip = 0.6 * (1 - (top_p if top_p == top_p else 0.0)) + 0.4 * ent_norm\n",
    "    return float(1.0 - race_skip) \n",
    "\n",
    "race_attention_by_race = df_oof.groupby(RACE_COL, group_keys=True).apply(_race_attention)\n",
    "race_attention_by_race.name = \"race_attention\"\n",
    "\n",
    "avg_race_attention    = float(np.nanmean(race_attention_by_race.values)) if len(race_attention_by_race) else float(\"nan\")\n",
    "median_race_attention = float(np.nanmedian(race_attention_by_race.values)) if len(race_attention_by_race) else float(\"nan\")\n",
    "\n",
    "avg_myoumi_score    = float(np.nanmean(df_oof[\"妙味スコア_oof\"].values))\n",
    "median_myoumi_score = float(np.nanmedian(df_oof[\"妙味スコア_oof\"].values))\n",
    "\n",
    "n_races  = int(race_attention_by_race.shape[0])\n",
    "n_horses = int(df_oof.shape[0])\n",
    "\n",
    "BASELINE_JSON = Path(\"03_baselines.json\")\n",
    "payload = {\n",
    "    \"avg_myoumi_score_oof\": avg_myoumi_score,\n",
    "    \"median_myoumi_score_oof\": median_myoumi_score,\n",
    "    \"avg_race_attention_oof\": avg_race_attention,\n",
    "    \"median_race_attention_oof\": median_race_attention,\n",
    "    \"n_races\": n_races,\n",
    "    \"n_horses\": n_horses,\n",
    "}\n",
    "BASELINE_JSON.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"✅ saved baselines -> {BASELINE_JSON} : {payload}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce293c",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b1a220",
   "metadata": {},
   "source": [
    "実装コストが低く試行が容易なため、Kerasでニューラルネットのベースラインを作成し、ツリー系（LGBM/CatBoost）と比較評価する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "610cddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb1bb0",
   "metadata": {},
   "source": [
    "## 推奨設定で学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f4343597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d9012a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_cols: ['騎手', '調教師', 'weather', 'race_type', 'ground_state', '競馬場', '所属', '月', '性']\n",
      "num_cols: ['枠番', '斤量', '単勝', '馬体重', 'course_len', '馬体重_増減', '年齢', '出走頭数', '相対枠位置', '大外枠', '最内枠', '着順_num', '複勝フラグ', '馬_直近3走_複勝率', '馬_直近5走_複勝率', '馬_直近10走_複勝率', '馬_直近5走_平均着順', '騎手_直近30走_複勝率', '調教師_直近50走_複勝率', '馬_コース適性_複勝率', '馬_距離適性_複勝率', 'コンビ_直近50走_複勝率', 'log単勝', '単勝ランク', '単勝pct', '単勝_最人気差', '単勝_2番人気差']\n",
      "base_w_pos (class_weight[1]): 45.042157470551764\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "LABEL_COL = \"HighPayoutHorse\"\n",
    "RACE_COL  = \"race_id\"\n",
    "\n",
    "train_df = results_df.iloc[tr_idx].copy()\n",
    "eval_df  = results_df.iloc[va_idx].copy()\n",
    "\n",
    "cat_cols = [c for c in train_df.select_dtypes(include=[\"object\"]).columns if c != RACE_COL]\n",
    "num_cols = [c for c in train_df.columns if c not in cat_cols + [LABEL_COL, RACE_COL]]\n",
    "\n",
    "print(\"cat_cols:\", cat_cols)\n",
    "print(\"num_cols:\", num_cols)\n",
    "\n",
    "def df_to_ds(df: pd.DataFrame, batch=1024, shuffle=True):\n",
    "    X = {}\n",
    "    for c in num_cols: X[c] = df[c].astype(\"float32\").values\n",
    "    for c in cat_cols: X[c] = df[c].astype(str).values\n",
    "    y = df[LABEL_COL].astype(\"float32\").values\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle: ds = ds.shuffle(len(df), seed=SEED)\n",
    "    return ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = df_to_ds(train_df, shuffle=True)\n",
    "valid_ds = df_to_ds(eval_df,  shuffle=False)\n",
    "\n",
    "pos = int(train_df[LABEL_COL].sum()); neg = len(train_df) - pos\n",
    "base_w_pos = neg / max(pos, 1)\n",
    "print(\"base_w_pos (class_weight[1]):\", base_w_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9004efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_JSON = \"03_keras_vocabs.json\"\n",
    "\n",
    "def _norm(s: pd.Series):\n",
    "    return normalize_text(s).fillna(\"NA\").astype(str)\n",
    "\n",
    "if not os.path.exists(VOCAB_JSON):\n",
    "    vocabs = {}\n",
    "    for c in results_df.select_dtypes(include=\"object\").columns:\n",
    "        if c in [LABEL_COL, RACE_COL]:\n",
    "            continue\n",
    "        vocabs[c] = sorted(_norm(results_df[c]).unique().tolist())\n",
    "    Path(VOCAB_JSON).write_text(json.dumps(vocabs, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"✅ saved vocab -> {VOCAB_JSON} (cols={len(vocabs)})\")\n",
    "\n",
    "VOCAB = json.loads(Path(VOCAB_JSON).read_text(encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "739c3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(alpha=0.25, gamma=2.0):\n",
    "    def _loss(y_true, y_pred):\n",
    "        eps = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)\n",
    "        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        w   = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)\n",
    "        return -tf.reduce_mean(w * tf.pow(1.0 - p_t, gamma) * tf.math.log(p_t))\n",
    "    return _loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae45c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = layers.Normalization()\n",
    "if num_cols:\n",
    "    norm_layer.adapt(train_df[num_cols].astype(\"float32\").values)\n",
    "\n",
    "lookup = {}\n",
    "vocab_sizes = {}\n",
    "for col in cat_cols:\n",
    "    v = VOCAB.get(col)\n",
    "    if v is None:\n",
    "        v = sorted(_norm(train_df[col]).unique().tolist())\n",
    "    lut = layers.StringLookup(vocabulary=v, output_mode=\"int\", num_oov_indices=1)\n",
    "    lookup[col] = lut\n",
    "    vocab_sizes[col] = lut.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "10b1bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_name(s: str) -> str:\n",
    "    return re.sub(r'[^A-Za-z0-9_.\\\\/>\\-]+', '_', str(s))\n",
    "\n",
    "NAME_MAP = {c: safe_name(c) for c in (num_cols + cat_cols)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "df71e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_safe_name_map(cols):\n",
    "    used = set()\n",
    "    name_map = {}\n",
    "    for i, c in enumerate(cols):\n",
    "        base = re.sub(r'[^A-Za-z0-9_.\\\\/>\\-]+', '_', str(c))\n",
    "        base = re.sub(r'_+', '_', base).strip('_')\n",
    "        if not base:\n",
    "            base = f\"col{i}\"\n",
    "        name = base\n",
    "        k = 2\n",
    "        while name in used:\n",
    "            name = f\"{base}_{k}\"\n",
    "            k += 1\n",
    "        used.add(name)\n",
    "        name_map[c] = name\n",
    "    return name_map\n",
    "\n",
    "NAME_MAP = build_safe_name_map(num_cols + cat_cols)\n",
    "\n",
    "assert len(set(NAME_MAP.values())) == len(NAME_MAP), \"NAME_MAP に重複があります\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_ds(df: pd.DataFrame, batch=1024, shuffle=True):\n",
    "    X = {}\n",
    "    for c in num_cols:\n",
    "        X[NAME_MAP[c]] = df[c].astype(\"float32\").values\n",
    "    for c in cat_cols:\n",
    "        X[NAME_MAP[c]] = normalize_text(df[c]).fillna(\"NA\").astype(str).values\n",
    "    y = df[LABEL_COL].astype(\"float32\").values\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(df), seed=SEED)\n",
    "    return ds.batch(batch).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "19cf8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hparams):\n",
    "    inputs = {}\n",
    "    for c in num_cols:\n",
    "        inputs[NAME_MAP[c]] = keras.Input(shape=(1,), name=NAME_MAP[c], dtype=tf.float32)\n",
    "    for c in cat_cols:\n",
    "        inputs[NAME_MAP[c]] = keras.Input(shape=(1,), name=NAME_MAP[c], dtype=tf.string)\n",
    "\n",
    "    feats = []\n",
    "    if num_cols:\n",
    "        x_num = layers.Concatenate(name=\"num_concat\")([inputs[NAME_MAP[c]] for c in num_cols])\n",
    "        x_num = norm_layer(x_num)\n",
    "        if hparams.get(\"num_bn\", False):\n",
    "            x_num = layers.BatchNormalization()(x_num)\n",
    "        feats.append(x_num)\n",
    "\n",
    "    emb_coef   = hparams.get(\"emb_coef\", 1.0)\n",
    "    emb_cap    = hparams.get(\"emb_cap\", 50)\n",
    "    emb_floor  = hparams.get(\"emb_floor\", 4)\n",
    "    emb_drop   = hparams.get(\"emb_dropout\", 0.0)\n",
    "\n",
    "    for c in cat_cols:\n",
    "        idx = lookup[c](inputs[NAME_MAP[c]])\n",
    "        dim = int(min(emb_cap, max(emb_floor, round(math.sqrt(vocab_sizes[c]) * emb_coef))))\n",
    "        emb = layers.Embedding(input_dim=vocab_sizes[c], output_dim=dim,\n",
    "                               name=f\"emb_{NAME_MAP[c]}\")(idx)\n",
    "        emb = layers.Reshape((dim,))(emb)\n",
    "        if emb_drop > 0:\n",
    "            emb = layers.Dropout(emb_drop)(emb)\n",
    "        feats.append(emb)\n",
    "\n",
    "    if not feats:\n",
    "        raise ValueError(\"特徴量がありません\")\n",
    "    x = feats[0] if len(feats)==1 else layers.Concatenate(name=\"all_concat\")(feats)\n",
    "\n",
    "    reg = keras.regularizers.l2(hparams.get(\"l2\", 1e-6))\n",
    "    act = hparams.get(\"activation\", \"relu\")\n",
    "    for width in hparams.get(\"hidden\", [256,128,64]):\n",
    "        x = layers.Dense(width, activation=act, kernel_regularizer=reg)(x)\n",
    "        if hparams.get(\"bn\", True):\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        dp = hparams.get(\"dropout\", 0.2)\n",
    "        if dp > 0:\n",
    "            x = layers.Dropout(dp)(x)\n",
    "\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "\n",
    "    if hparams.get(\"use_focal\", True):\n",
    "        loss_fn = focal_loss(alpha=hparams.get(\"focal_alpha\", 0.25),\n",
    "                             gamma=hparams.get(\"focal_gamma\", 2.0))\n",
    "    else:\n",
    "        loss_fn = \"binary_crossentropy\"\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=hparams.get(\"lr\", 1e-3))\n",
    "    model.compile(optimizer=opt, loss=loss_fn,\n",
    "                  metrics=[keras.metrics.AUC(name=\"roc_auc\"),\n",
    "                           keras.metrics.AUC(curve=\"PR\", name=\"pr_auc\")])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3bed0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_topk_precision(\n",
    "    df: pd.DataFrame,\n",
    "    prob: np.ndarray | list,\n",
    "    k: int = 1,\n",
    "    ycol: str = LABEL_COL,\n",
    "    race_col: str = RACE_COL,\n",
    "    min_prob: float | None = None, \n",
    ") -> float:\n",
    "    \"\"\"各レースで予測確率の上位k頭の中に、少なくとも1頭ラベル=1が含まれる割合（レース単位Hit@K）。\"\"\"\n",
    "    prob = np.asarray(prob)\n",
    "    if len(prob) != len(df):\n",
    "        raise ValueError(f\"prob長さ({len(prob)})とdf長さ({len(df)})が一致していません。\")\n",
    "\n",
    "    t = df[[race_col, ycol]].copy()\n",
    "    t[\"prob\"] = prob\n",
    "\n",
    "    if min_prob is not None:\n",
    "        ok_races = (t.groupby(race_col)[\"prob\"].max() >= min_prob)\n",
    "        ok_races = ok_races[ok_races].index\n",
    "        t = t[t[race_col].isin(ok_races)]\n",
    "\n",
    "    if len(t) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    topk = (t.sort_values([race_col, \"prob\"], ascending=[True, False])\n",
    "              .groupby(race_col, as_index=False)\n",
    "              .head(k))\n",
    "\n",
    "    return float(topk.groupby(race_col)[ycol].max().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c401e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - loss: nan - pr_auc: 0.0210 - roc_auc: 0.4886 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000\n",
      "Epoch 2/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: nan - pr_auc: 0.0219 - roc_auc: 0.5037 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000\n",
      "Epoch 3/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: nan - pr_auc: 0.0221 - roc_auc: 0.5114 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000\n",
      "Epoch 4/10\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: nan - pr_auc: 0.0230 - roc_auc: 0.5039 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000\n",
      "\n",
      "=== Keras NN 総合評価（検証） ===\n",
      "サンプル数         : 18604（陽性 440, 陽性率 2.37%）\n",
      "損失               : Focal\n",
      "ROC-AUC            : 0.5000\n",
      "PR-AUC             : 0.0237\n",
      "Brier              : 0.0885\n",
      "Race Hit@1 / Hit@2 : 0.032 / 0.058\n",
      "Lift@10%           : 1.00x\n",
      "Best val_pr_auc    : 0.0237 @ epoch 1（実行 4epochs）\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "hp0 = dict(\n",
    "    emb_coef=1.0, emb_cap=32, emb_floor=4, emb_dropout=0.0,\n",
    "    hidden=[256,128,64], dropout=0.2, bn=True, num_bn=False,\n",
    "    activation=\"relu\", l2=1e-5,\n",
    "    use_focal=True, focal_alpha=0.25, focal_gamma=2.0, lr=1e-3,\n",
    "    batch_size=1024, epochs=10, patience=3,\n",
    ")\n",
    "\n",
    "model_keras = build_model(hp0)\n",
    "\n",
    "cw1 = min(base_w_pos, base_w_pos*0.3) if hp0[\"use_focal\"] else base_w_pos\n",
    "class_weight = {0: 1.0, 1: cw1}\n",
    "\n",
    "train_ds = df_to_ds(train_df, batch=hp0[\"batch_size\"], shuffle=True)\n",
    "valid_ds = df_to_ds(eval_df,  batch=hp0[\"batch_size\"], shuffle=False)\n",
    "\n",
    "cb = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_pr_auc\", mode=\"max\",\n",
    "                                  patience=hp0[\"patience\"], restore_best_weights=True)\n",
    "]\n",
    "\n",
    "# 学習\n",
    "hist = model_keras.fit(\n",
    "    train_ds, validation_data=valid_ds,\n",
    "    epochs=hp0[\"epochs\"], class_weight=class_weight,\n",
    "    callbacks=cb, verbose=1\n",
    ")\n",
    "\n",
    "# 予測＆指標\n",
    "p_valid = model_keras.predict(valid_ds, verbose=0).ravel()\n",
    "y_true  = eval_df[LABEL_COL].to_numpy().astype(int)\n",
    "\n",
    "roc  = roc_auc_score(y_true, p_valid)\n",
    "pr   = average_precision_score(y_true, p_valid)\n",
    "br   = brier_score_loss(y_true, p_valid)\n",
    "\n",
    "top1 = top2 = None\n",
    "if \"race_topk_precision\" in globals():\n",
    "    top1 = float(race_topk_precision(eval_df, p_valid, k=1))\n",
    "    top2 = float(race_topk_precision(eval_df, p_valid, k=2))\n",
    "\n",
    "thr  = np.quantile(p_valid, 0.9)\n",
    "lift = float((y_true[p_valid >= thr].mean()) / y_true.mean())\n",
    "\n",
    "val_hist = np.array(hist.history.get(\"val_pr_auc\", []), dtype=float)\n",
    "best_ep  = int(np.nanargmax(val_hist) + 1) if len(val_hist) else None\n",
    "best_val = float(np.nanmax(val_hist)) if len(val_hist) else float(\"nan\")\n",
    "trained_epochs = len(hist.history.get(\"loss\", []))\n",
    "\n",
    "n = len(y_true); pos = int(y_true.sum())\n",
    "print(\"\\n=== Keras NN 総合評価（検証） ===\")\n",
    "print(f\"サンプル数         : {n}（陽性 {pos}, 陽性率 {pos/n:.2%}）\")\n",
    "print(f\"損失               : {'Focal' if hp0.get('use_focal', True) else 'BinaryCE'}\")\n",
    "print(f\"ROC-AUC            : {roc:.4f}\")\n",
    "print(f\"PR-AUC             : {pr:.4f}\")\n",
    "print(f\"Brier              : {br:.4f}\")\n",
    "if top1 is not None:\n",
    "    print(f\"Race Hit@1 / Hit@2 : {top1:.3f} / {top2:.3f}\")\n",
    "print(f\"Lift@10%           : {lift:.2f}x\")\n",
    "if best_ep is not None:\n",
    "    print(f\"Best val_pr_auc    : {best_val:.4f} @ epoch {best_ep}（実行 {trained_epochs}epochs）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e679ce8",
   "metadata": {},
   "source": [
    "ランダムと同値のスコアとなった。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f4133",
   "metadata": {},
   "source": [
    "## Optunaでチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e804e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    h = dict(\n",
    "        emb_coef    = trial.suggest_float(\"emb_coef\", 0.7, 1.5),\n",
    "        emb_cap     = trial.suggest_int(\"emb_cap\", 16, 64),\n",
    "        emb_floor   = 4,\n",
    "        emb_dropout = trial.suggest_float(\"emb_dropout\", 0.0, 0.2),\n",
    "\n",
    "        hidden      = [trial.suggest_int(\"h1\",128,512,log=True),\n",
    "                       trial.suggest_int(\"h2\",64,256,log=True),\n",
    "                       trial.suggest_int(\"h3\",32,128,log=True)],\n",
    "        dropout     = trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
    "        bn          = trial.suggest_categorical(\"bn\",[True, False]),\n",
    "        num_bn      = trial.suggest_categorical(\"num_bn\",[False, True]),\n",
    "        activation  = trial.suggest_categorical(\"activation\", [\"relu\", \"gelu\", \"selu\"]),\n",
    "        l2          = trial.suggest_float(\"l2\", 1e-7, 1e-3, log=True),\n",
    "\n",
    "        use_focal   = trial.suggest_categorical(\"use_focal\", [True, False]),\n",
    "        focal_alpha = trial.suggest_float(\"focal_alpha\", 0.1, 0.5),\n",
    "        focal_gamma = trial.suggest_float(\"focal_gamma\", 1.0, 3.0),\n",
    "        lr          = trial.suggest_float(\"lr\", 5e-4, 3e-3, log=True),\n",
    "\n",
    "        batch_size  = trial.suggest_categorical(\"batch_size\", [512, 1024, 2048]),\n",
    "        epochs      = 30,\n",
    "        patience    = 6,\n",
    "    )\n",
    "\n",
    "    model = build_model(h)\n",
    "\n",
    "    cw1 = min(base_w_pos, base_w_pos*0.3) if h[\"use_focal\"] else base_w_pos\n",
    "    class_weight = {0:1.0, 1:cw1}\n",
    "\n",
    "    train_ds_trial = df_to_ds(train_df, batch=h[\"batch_size\"], shuffle=True)\n",
    "    valid_ds_trial = df_to_ds(eval_df,  batch=h[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    cb = [\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_pr_auc\", mode=\"max\",\n",
    "                                      patience=h[\"patience\"], restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_pr_auc\", mode=\"max\",\n",
    "                                          factor=0.5, patience=2, min_lr=1e-5),\n",
    "    ]\n",
    "\n",
    "    hist = model.fit(\n",
    "        train_ds_trial, validation_data=valid_ds_trial,\n",
    "        epochs=h[\"epochs\"], class_weight=class_weight,\n",
    "        verbose=0, callbacks=cb\n",
    "    )\n",
    "\n",
    "    pr_hist = hist.history.get(\"val_pr_auc\", [])\n",
    "    return float(np.max(pr_hist)) if len(pr_hist) else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ca08dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0236508: 100%|██████████| 100/100 [44:42<00:00, 26.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PR-AUC (NN): 0.023650826886296272\n",
      "Best params (NN):\n",
      "  emb_coef: 0.8133115677197165\n",
      "  emb_cap: 20\n",
      "  emb_dropout: 0.04809503919384766\n",
      "  h1: 448\n",
      "  h2: 234\n",
      "  h3: 59\n",
      "  dropout: 0.11760539695088079\n",
      "  bn: True\n",
      "  num_bn: True\n",
      "  activation: relu\n",
      "  l2: 1.389407409422714e-05\n",
      "  use_focal: False\n",
      "  focal_alpha: 0.4009645431058436\n",
      "  focal_gamma: 1.8200654975432533\n",
      "  lr: 0.0013329271527386614\n",
      "  batch_size: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "study_nn = optuna.create_study(direction=\"maximize\", study_name=\"keras_nn_pr_auc\")\n",
    "study_nn.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(\"Best PR-AUC (NN):\", study_nn.best_value)\n",
    "print(\"Best params (NN):\")\n",
    "for k,v in study_nn.best_params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494739f1",
   "metadata": {},
   "source": [
    "## ベストパラメータで再学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "97e51704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - loss: nan - pr_auc: 0.0217 - roc_auc: 0.4966 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000 - learning_rate: 0.0013\n",
      "Epoch 2/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: nan - pr_auc: 0.0211 - roc_auc: 0.4898 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000 - learning_rate: 0.0013\n",
      "Epoch 3/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - loss: nan - pr_auc: 0.0208 - roc_auc: 0.4858 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000 - learning_rate: 0.0013\n",
      "Epoch 4/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: nan - pr_auc: 0.0214 - roc_auc: 0.4970 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000 - learning_rate: 6.6646e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: nan - pr_auc: 0.0215 - roc_auc: 0.4945 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000 - learning_rate: 6.6646e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step - loss: nan - pr_auc: 0.0217 - roc_auc: 0.5001 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000 - learning_rate: 3.3323e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step - loss: nan - pr_auc: 0.0208 - roc_auc: 0.4887 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000 - learning_rate: 3.3323e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - loss: nan - pr_auc: 0.0219 - roc_auc: 0.5004 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000 - learning_rate: 1.6662e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step - loss: nan - pr_auc: 0.0217 - roc_auc: 0.5002 - val_loss: nan - val_pr_auc: 0.0237 - val_roc_auc: 0.5000 - learning_rate: 1.6662e-04\n",
      "[Keras NN tuned] ROC-AUC=0.5000  PR-AUC=0.0237  Brier=0.2617\n",
      "Top1=0.0324  Top2=0.0582  Lift@10%=1.00x\n",
      "✅ Saved tuned NN -> 03_keras_nn_best.keras\n",
      "Saved Optuna study results.\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "best_hp = study_nn.best_params\n",
    "best_hp[\"epochs\"] = 100\n",
    "best_hp[\"patience\"] = 8\n",
    "\n",
    "model_best = build_model(best_hp)\n",
    "\n",
    "cw1 = min(base_w_pos, base_w_pos*0.3) if best_hp.get(\"use_focal\", True) else base_w_pos\n",
    "class_weight = {0:1.0, 1:cw1}\n",
    "\n",
    "train_ds_best = df_to_ds(train_df, batch=best_hp.get(\"batch_size\", 1024), shuffle=True)\n",
    "valid_ds_best = df_to_ds(eval_df,  batch=best_hp.get(\"batch_size\", 1024), shuffle=False)\n",
    "\n",
    "cb = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_pr_auc\", mode=\"max\",\n",
    "                                  patience=best_hp[\"patience\"], restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_pr_auc\", mode=\"max\",\n",
    "                                      factor=0.5, patience=2, min_lr=1e-5),\n",
    "]\n",
    "history = model_best.fit(\n",
    "    train_ds_best, validation_data=valid_ds_best,\n",
    "    epochs=best_hp[\"epochs\"], class_weight=class_weight,\n",
    "    verbose=1, callbacks=cb\n",
    ")\n",
    "\n",
    "p_valid = model_best.predict(valid_ds_best, verbose=0).ravel()\n",
    "y_true  = eval_df[LABEL_COL].to_numpy().astype(int)\n",
    "\n",
    "roc  = roc_auc_score(y_true, p_valid)\n",
    "pr   = average_precision_score(y_true, p_valid)\n",
    "br   = brier_score_loss(y_true, p_valid)\n",
    "print(f\"[Keras NN tuned] ROC-AUC={roc:.4f}  PR-AUC={pr:.4f}  Brier={br:.4f}\")\n",
    "\n",
    "def race_topk_precision(df, prob, k=1, ycol=LABEL_COL, race_col=RACE_COL):\n",
    "    t = df[[race_col, ycol]].copy()\n",
    "    t[\"prob\"] = prob\n",
    "    return t.groupby(race_col).apply(\n",
    "        lambda g: g.sort_values(\"prob\", ascending=False).head(k)[ycol].max()\n",
    "    ).mean()\n",
    "\n",
    "top1 = race_topk_precision(eval_df, p_valid, k=1)\n",
    "top2 = race_topk_precision(eval_df, p_valid, k=2)\n",
    "thr  = np.quantile(p_valid, 0.9)\n",
    "lift = (y_true[p_valid >= thr].mean()) / y_true.mean()\n",
    "print(f\"Top1={top1:.4f}  Top2={top2:.4f}  Lift@10%={lift:.2f}x\")\n",
    "\n",
    "# 保存\n",
    "nn_path = \"03_keras_nn_best.keras\"\n",
    "model_best.save(nn_path)\n",
    "print(f\"✅ Saved tuned NN -> {nn_path}\")\n",
    "\n",
    "# Optunaの履歴保存\n",
    "joblib.dump(study_nn, \"03_optuna_study_keras_nn.pkl\")\n",
    "study_nn.trials_dataframe().to_csv(\"03_optuna_study_keras_nn_trials.csv\", index=False)\n",
    "print(\"Saved Optuna study results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda76ac",
   "metadata": {},
   "source": [
    "OOV(=Out Of Vocabulary)を確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc06632e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred stats: 0.51211 0.51211 0.51211 1.1920929e-07\n",
      "騎手: vocab=236, OOV=0.0%\n",
      "調教師: vocab=263, OOV=0.0%\n",
      "weather: vocab=7, OOV=5.3%\n",
      "race_type: vocab=4, OOV=48.9%\n",
      "ground_state: vocab=5, OOV=4.2%\n",
      "競馬場: vocab=11, OOV=10.5%\n",
      "所属: vocab=5, OOV=0.1%\n",
      "月: vocab=22, OOV=4.7%\n",
      "性: vocab=4, OOV=4.5%\n",
      "race_id in cat_cols? False\n"
     ]
    }
   ],
   "source": [
    "print(\"pred stats:\", np.min(p_valid), np.median(p_valid), np.max(p_valid), np.std(p_valid))\n",
    "\n",
    "for c, lut in lookup.items():\n",
    "    ids = lut(tf.constant(eval_df[c].astype(str).values))\n",
    "    ids = ids.numpy().ravel()\n",
    "    oov_id = 1\n",
    "    oov_ratio = (ids == oov_id).mean()\n",
    "    print(f\"{c}: vocab={vocab_sizes[c]}, OOV={oov_ratio:.1%}\")\n",
    "\n",
    "print(\"race_id in cat_cols?\", RACE_COL in cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d4ff7",
   "metadata": {},
   "source": [
    "race_typeなどでOOVが残り、検証ではPR-AUC≒陽性率・ROC-AUC≈0.5とツリー系に劣後。  \n",
    "OOV処理や追加学習は打ち切り、最終は LGBM＋CatBoost を採用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
